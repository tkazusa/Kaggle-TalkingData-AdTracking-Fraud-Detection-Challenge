{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%sh\n",
    "\n",
    "conda install -y -q -c conda-forge lightgbm\n",
    "conda install -y -q -c conda-forge gensim\n",
    "pip install kaggle\n",
    "\n",
    "if [ ! -e /home/ec2-user/SageMaker/TalkingDataAdTrackingFraudDetectionChallenge/input/* ]; then    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm                  2.2.2            py36hf484d3e_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list | grep gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from gensim import corpora, models\n",
    "\n",
    "DATADIR = Path('./input')\n",
    "\n",
    "tr_path = DATADIR / 'train.csv'\n",
    "test_path = DATADIR / 'test.csv'\n",
    "\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os',\n",
    "              'channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandasデータ型指定によるメモリ使用量の削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データサイズが大きいので`float64`や`int64`をなるべく使わずに最適な型を選ぶように変換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "        \n",
    "    Args:\n",
    "        df (pd.DataFrame): pd.DataFrame to be reduced memory usage.\n",
    "    Regurns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"create a dataframe and optimize its memory usage\n",
    "    Args:\n",
    "        filepath (str): Path to csv file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed for memory usage reduction.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, parse_dates=True, keep_date_col=True).head(10000)\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.61 MB\n",
      "Memory usage after optimization is: 0.18 MB\n",
      "Decreased by 71.1%\n"
     ]
    }
   ],
   "source": [
    "# train_sm = load_data(tr_path)\n",
    "# test_sm = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sm.to_csv(DATADIR / 'train_sm.csv', index=False)\n",
    "# test_sm.to_csv(DATADIR / 'test_sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and bind train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATADIR / 'train_sm.csv', parse_dates=True, keep_date_col=True)\n",
    "len_train = len(train)\n",
    "\n",
    "test = pd.read_csv(DATADIR / 'test_sm.csv', parse_dates=True, keep_date_col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_tr_test(train: pd.DataFrame, test: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Bind train and test data for features engineering.\n",
    "    Args:\n",
    "        train (pd.DataFrame): train data.\n",
    "        test (pd.DataFrame): test data.\n",
    "    Returns:\n",
    "        data (pd.DataFrame): binded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    len_train = len(train)\n",
    "    print('The initial size of the train set is', len_train)\n",
    "    print('Binding the training and test set together...')\n",
    "    data = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial size of the train set is 10000\n",
      "Binding the training and test set together...\n"
     ]
    }
   ],
   "source": [
    "data = bind_tr_test(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "click_timeは`2017-11-10 04:00:00`の形なので日付と時間の特徴量を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(data: pd.DataFrame):\n",
    "    print(\"Creating new time features: 'hour' and 'day'...\")\n",
    "    data['hour'] = pd.to_datetime(data.click_time).dt.hour.astype('uint8')\n",
    "    data['day'] = pd.to_datetime(data.click_time).dt.day.astype('uint8')\n",
    "\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new time features: 'hour' and 'day'...\n"
     ]
    }
   ],
   "source": [
    "data = create_time_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ベーシックな処理\n",
    "  - five raw categorical features (ip, os, app, channel, device)  （単純に型をカテゴリ化）\n",
    "  - time categorical features (day, hour) \n",
    "  - some count features \n",
    "- web広告配信データ特有の特徴量\n",
    "  - five raw categorical features (ip, os, app, channel, device) に対し、以下の特徴量を作成 (全組み合わせ2^5 -1 = 31通り)\n",
    "  - click count within next one/six hours  (直後1 or 6時間以内のクリック数)\n",
    "  - forward/backward click time delta  (前後クリックまでの時差)\n",
    "  - average attributed ratio of past click (過去のCVレート)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_channels_features(data):\n",
    "    print(\"Creating new count features: 'n_channels', 'ip_app_count', 'ip_app_os_count'...\")\n",
    "\n",
    "    print('Computing the number of channels associated with ')\n",
    "    print('a given IP address within each hour...')\n",
    "    print('一時間の中でIPアドレス毎のチャネル数を数えている')\n",
    "    n_chans = data[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'n_channels'})\n",
    "    print('Merging the channels data with the main data set...')\n",
    "    data = data.merge(n_chans, on=['ip', 'day', 'hour'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['n_channels'].astype('uint16').to_csv(\n",
    "        DATADIR/'n_channels.csv', header=False)\n",
    "    print(\"Saving the data\")\n",
    "    data.drop(['n_channels'], axis=1)\n",
    "\n",
    "    print('Computing the number of channels associated with ')\n",
    "    print('a given IP address and app...')\n",
    "    print('IPアドレス毎/app毎のチャネル数を数えている')\n",
    "    n_chans = data[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_count'})\n",
    "    print('Merging the channels data with the main data set...')\n",
    "    data = data.merge(n_chans, on=['ip', 'app'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_count'].astype('uint16').to_csv(\n",
    "        DATADIR/'ip_app_count.csv', header=False)\n",
    "    print(\"Saving the data\")\n",
    "    data.drop(['ip_app_count'], axis=1)\n",
    "\n",
    "    print('Computing the number of channels associated with ')\n",
    "    print('a given IP address, app, and os...')\n",
    "    print('IPアドレス毎/app毎/os毎のチャネル数を数えている')\n",
    "    n_chans = data[['ip', 'app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_os_count'})\n",
    "    print('Merging the channels data with the main data set...')\n",
    "    data = data.merge(n_chans, on=['ip', 'app', 'os'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_os_count'].astype('uint16').to_csv(\n",
    "        DATADIR/'ip_app_os_count.csv', header=False)\n",
    "    print(\"Saving the data\")\n",
    "    data.drop(['ip_app_os_count'], axis=1)\n",
    "\n",
    "    del data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new count features: 'n_channels', 'ip_app_count', 'ip_app_os_count'...\n",
      "Computing the number of channels associated with \n",
      "a given IP address within each hour...\n",
      "一時間の中でIPアドレス毎のチャネル数を数えている\n",
      "Merging the channels data with the main data set...\n",
      "Saving the data\n",
      "Computing the number of channels associated with \n",
      "a given IP address and app...\n",
      "IPアドレス毎/app毎のチャネル数を数えている\n",
      "Merging the channels data with the main data set...\n",
      "Saving the data\n",
      "Computing the number of channels associated with \n",
      "a given IP address, app, and os...\n",
      "IPアドレス毎/app毎/os毎のチャネル数を数えている\n",
      "Merging the channels data with the main data set...\n",
      "Saving the data\n"
     ]
    }
   ],
   "source": [
    "create_count_channels_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAを用いたカテゴリカルデータの埋め込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のデータはipやosなど、多数のカテゴリをを抱える特徴量がある。それ単体でも特徴なり得るが、任意のカテゴリがどのような意味を持つかについて、他の特徴の各カテゴリとの共起から情報を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LDA_features(df: pd.DaraFrame, num_topics: int, column_pair: tupple)-> None:\n",
    "    \"\"\" Create LDA feateures calculated with a pair of categorical features\n",
    "    Args\n",
    "        df:\n",
    "        num_topics:\n",
    "        column_pair \n",
    "    \"\"\"\n",
    "    col1, col2 = column_pair\n",
    "    print('pair of %s & %s' % (col1, col2))\n",
    "    tmp_dict = {}\n",
    "    for v_col1, v_col2 in zip(data[col1], data[col2]):\n",
    "        tmp_dict.setdefault(v_col1, []).append(str(v_col2))\n",
    "\n",
    "    col1_list = list(tmp_dict.keys())\n",
    "    col2s_of_col1s_list = [[' '.join(tmp_dict[tokun])] for tokun in col1_list]\n",
    "\n",
    "    dictionary = corpora.Dictionary(col2s_of_col1s_list)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in col2s_of_col1s_list]\n",
    "    print('---Start learning LDA model---')\n",
    "\n",
    "    model = models.LdaModel(corpus,\n",
    "                            num_topics=num_topics,\n",
    "                            id2word=dictionary,\n",
    "                            random_state=3655\n",
    "                            )\n",
    "\n",
    "    print('---Saving the model---')\n",
    "    features = np.array(model.get_document_topics(\n",
    "        corpus, minimum_probability=0))[:, :, 1]\n",
    "\n",
    "    column_name_list = [\"lda_%s_%s_\" % (col1, col2) + str(i) for i in range(5)]\n",
    "\n",
    "    df_features = pd.DataFrame(features, columns=column_name_list)\n",
    "    df_features[col1] = col1_list\n",
    "\n",
    "    print(\"---Merging data---\")\n",
    "    data = pd.merge(data, df_features, on=col1, how='left')\n",
    "    del df_features\n",
    "    gc.collect()\n",
    "\n",
    "    datapath = \"lda_\" + col1 + \"_\" + col2 + \".csv\"\n",
    "    data[column_name_list].to_csv(DATADIR/datapath)\n",
    "\n",
    "    print(\"Shape of merged data is %s %s \" % data[column_name_list].shape)\n",
    "\n",
    "\n",
    "def get_column_pairs(columns):\n",
    "    return [(col1, col2) for col1, col2 in itertools.product(columns, repeat=2) if col1 != col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair of ip & app\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_ip_app_0  lda_ip_app_1  lda_ip_app_2  lda_ip_app_3  lda_ip_app_4  \\\n",
      "0      0.599627      0.100010      0.100359      0.100001      0.100002   \n",
      "1      0.100068      0.599767      0.100004      0.100156      0.100004   \n",
      "2      0.599604      0.100010      0.100383      0.100001      0.100002   \n",
      "3      0.100351      0.100358      0.100371      0.100342      0.598577   \n",
      "4      0.599627      0.100010      0.100359      0.100001      0.100002   \n",
      "\n",
      "       ip  \n",
      "0   83230  \n",
      "1   17357  \n",
      "2   35810  \n",
      "3   45745  \n",
      "4  161007  \n",
      "shape of merged data is 20000 5 \n",
      "pair of ip & os\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_ip_os_0  lda_ip_os_1  lda_ip_os_2  lda_ip_os_3  lda_ip_os_4      ip\n",
      "0     0.100000     0.100023     0.599975     0.100000     0.100000   83230\n",
      "1     0.599369     0.100582     0.100004     0.100042     0.100004   17357\n",
      "2     0.100000     0.100023     0.599975     0.100000     0.100000   35810\n",
      "3     0.100369     0.100424     0.100391     0.100426     0.598391   45745\n",
      "4     0.100000     0.100023     0.599975     0.100000     0.100000  161007\n",
      "shape of merged data is 20000 5 \n",
      "pair of ip & channel\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_ip_channel_0  lda_ip_channel_1  lda_ip_channel_2  lda_ip_channel_3  \\\n",
      "0          0.100012          0.598350          0.101596          0.100032   \n",
      "1          0.101124          0.100020          0.100478          0.598356   \n",
      "2          0.100012          0.598347          0.101598          0.100032   \n",
      "3          0.598585          0.100348          0.100327          0.100365   \n",
      "4          0.100012          0.598340          0.101606          0.100032   \n",
      "\n",
      "   lda_ip_channel_4      ip  \n",
      "0          0.100011   83230  \n",
      "1          0.100022   17357  \n",
      "2          0.100011   35810  \n",
      "3          0.100376   45745  \n",
      "4          0.100011  161007  \n",
      "shape of merged data is 20000 5 \n",
      "pair of app & ip\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_app_ip_0  lda_app_ip_1  lda_app_ip_2  lda_app_ip_3  lda_app_ip_4  app\n",
      "0      0.100031      0.100039      0.100034      0.599866      0.100031    3\n",
      "1      0.100030      0.100036      0.100030      0.100027      0.599877   14\n",
      "2      0.100027      0.100033      0.599891      0.100024      0.100025   64\n",
      "3      0.599882      0.100036      0.100030      0.100025      0.100027   18\n",
      "4      0.100026      0.100032      0.599891      0.100024      0.100026    6\n",
      "shape of merged data is 20000 5 \n",
      "pair of app & os\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_app_os_0  lda_app_os_1  lda_app_os_2  lda_app_os_3  lda_app_os_4  app\n",
      "0      0.100042      0.100042      0.100029      0.599855      0.100033    3\n",
      "1      0.100040      0.100038      0.100024      0.100026      0.599871   14\n",
      "2      0.100043      0.100038      0.100026      0.599862      0.100031   64\n",
      "3      0.100040      0.100036      0.100025      0.100027      0.599872   18\n",
      "4      0.599912      0.100027      0.100019      0.100020      0.100022    6\n",
      "shape of merged data is 20000 5 \n",
      "pair of app & channel\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_app_channel_0  lda_app_channel_1  lda_app_channel_2  lda_app_channel_3  \\\n",
      "0           0.100037           0.100042           0.100034           0.599862   \n",
      "1           0.100040           0.100043           0.100033           0.599857   \n",
      "2           0.100040           0.100044           0.100033           0.599854   \n",
      "3           0.599901           0.100033           0.100024           0.100021   \n",
      "4           0.100025           0.599916           0.100022           0.100019   \n",
      "\n",
      "   lda_app_channel_4  app  \n",
      "0           0.100026    3  \n",
      "1           0.100027   14  \n",
      "2           0.100029   64  \n",
      "3           0.100021   18  \n",
      "4           0.100018    6  \n",
      "shape of merged data is 20000 5 \n",
      "pair of os & ip\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_os_ip_0  lda_os_ip_1  lda_os_ip_2  lda_os_ip_3  lda_os_ip_4  os\n",
      "0     0.100029     0.100033     0.599880     0.100025     0.100033  13\n",
      "1     0.599884     0.100034     0.100028     0.100024     0.100030  19\n",
      "2     0.599885     0.100031     0.100028     0.100025     0.100030  16\n",
      "3     0.599885     0.100032     0.100028     0.100025     0.100030  23\n",
      "4     0.100024     0.599907     0.100023     0.100021     0.100025  22\n",
      "shape of merged data is 20000 5 \n",
      "pair of os & app\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_os_app_0  lda_os_app_1  lda_os_app_2  lda_os_app_3  lda_os_app_4  os\n",
      "0      0.100033      0.100037      0.100034      0.599864      0.100032  13\n",
      "1      0.599884      0.100032      0.100029      0.100025      0.100030  19\n",
      "2      0.100029      0.100032      0.599890      0.100024      0.100026  16\n",
      "3      0.599889      0.100032      0.100028      0.100024      0.100027  23\n",
      "4      0.100028      0.100031      0.599889      0.100024      0.100028  22\n",
      "shape of merged data is 20000 5 \n",
      "pair of os & channel\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_os_channel_0  lda_os_channel_1  lda_os_channel_2  lda_os_channel_3  \\\n",
      "0          0.100031          0.100032          0.599871          0.100030   \n",
      "1          0.100031          0.100031          0.100026          0.599881   \n",
      "2          0.100030          0.100031          0.100028          0.599881   \n",
      "3          0.599888          0.100028          0.100025          0.100028   \n",
      "4          0.100027          0.100028          0.100024          0.100025   \n",
      "\n",
      "   lda_os_channel_4  os  \n",
      "0          0.100035  13  \n",
      "1          0.100031  19  \n",
      "2          0.100030  16  \n",
      "3          0.100030  23  \n",
      "4          0.599896  22  \n",
      "shape of merged data is 20000 5 \n",
      "pair of channel & ip\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_channel_ip_0  lda_channel_ip_1  lda_channel_ip_2  lda_channel_ip_3  \\\n",
      "0          0.100033          0.100033          0.599863          0.100038   \n",
      "1          0.100029          0.100030          0.100024          0.100034   \n",
      "2          0.599889          0.100028          0.100024          0.100032   \n",
      "3          0.599889          0.100028          0.100024          0.100032   \n",
      "4          0.100029          0.100028          0.100026          0.100032   \n",
      "\n",
      "   lda_channel_ip_4  channel  \n",
      "0          0.100033      379  \n",
      "1          0.599883      478  \n",
      "2          0.100027      459  \n",
      "3          0.100028      376  \n",
      "4          0.599885      120  \n",
      "shape of merged data is 20000 5 \n",
      "pair of channel & app\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_channel_app_0  lda_channel_app_1  lda_channel_app_2  lda_channel_app_3  \\\n",
      "0           0.100032           0.100030           0.100030           0.599872   \n",
      "1           0.599883           0.100028           0.100028           0.100027   \n",
      "2           0.599890           0.100026           0.100026           0.100025   \n",
      "3           0.100034           0.100030           0.100029           0.599870   \n",
      "4           0.100031           0.100027           0.100030           0.599877   \n",
      "\n",
      "   lda_channel_app_4  channel  \n",
      "0           0.100036      379  \n",
      "1           0.100034      478  \n",
      "2           0.100032      459  \n",
      "3           0.100037      376  \n",
      "4           0.100035      120  \n",
      "shape of merged data is 20000 5 \n",
      "pair of channel & os\n",
      "Start learning LDA model\n",
      "Saving the model\n",
      "---merging data---\n",
      "   lda_channel_os_0  lda_channel_os_1  lda_channel_os_2  lda_channel_os_3  \\\n",
      "0          0.100031          0.100029          0.100027          0.599878   \n",
      "1          0.599883          0.100028          0.100027          0.100028   \n",
      "2          0.599890          0.100026          0.100025          0.100026   \n",
      "3          0.100033          0.100029          0.100027          0.599875   \n",
      "4          0.100030          0.100026          0.100027          0.599882   \n",
      "\n",
      "   lda_channel_os_4  channel  \n",
      "0          0.100035      379  \n",
      "1          0.100035      478  \n",
      "2          0.100033      459  \n",
      "3          0.100036      376  \n",
      "4          0.100034      120  \n",
      "shape of merged data is 20000 5 \n"
     ]
    }
   ],
   "source": [
    "columns = ['ip', 'app', 'os', 'channel']\n",
    "column_pairs = get_column_pairs(columns)\n",
    "\n",
    "for pair in column_pairs:\n",
    "    create_LDA_features(data, pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不均衡データに対するNegative donwsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルサイズの削減とクラス不均衡な二値分類への対応としてNegativeDownSamplingを使用した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_down_sampling(data, random_state, target_variable):\n",
    "    positive_data = data[data[target_variable] == 1]\n",
    "    positive_ratio = float(len(positive_data)) / len(data)\n",
    "    negative_data = data[data[target_variable] == 0].sample(\n",
    "        frac=positive_ratio / (1 - positive_ratio), random_state=random_state)\n",
    "    return pd.concat([positive_data, negative_data])\n",
    "\n",
    "def train_test_split(data, len_train):\n",
    "    train = data[:len_train]\n",
    "    test = data[len_train:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23830"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(data, len_train)\n",
    "sampled_train = negative_down_sampling(train, target_variable='is_attributed', random_state=3655)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ip  app  device  os  channel           click_time  \\\n",
      "103   204158   35       1  13       21  2017-11-06 15:41:07   \n",
      "1504   29692    9       1  22      215  2017-11-06 16:00:02   \n",
      "1798   64516   35       1  13       21  2017-11-06 16:00:02   \n",
      "2102  172429   35       1  46      274  2017-11-06 16:00:03   \n",
      "3056  199085   35       1  13      274  2017-11-06 16:00:04   \n",
      "\n",
      "          attributed_time  is_attributed  click_id  hour  day  \n",
      "103   2017-11-07 08:17:19            1.0       NaN    15    6  \n",
      "1504  2017-11-07 10:05:22            1.0       NaN    16    6  \n",
      "1798  2017-11-06 23:40:50            1.0       NaN    16    6  \n",
      "2102  2017-11-07 00:55:29            1.0       NaN    16    6  \n",
      "3056  2017-11-06 23:04:54            1.0       NaN    16    6  \n",
      "(46, 11)\n",
      "================================================================================\n",
      "           ip  app  device  os  channel           click_time attributed_time  \\\n",
      "10000    5744    9       1   3      107  2017-11-10 04:00:00             NaN   \n",
      "10001  119901    9       1   3      466  2017-11-10 04:00:00             NaN   \n",
      "10002   72287   21       1  19      128  2017-11-10 04:00:00             NaN   \n",
      "10003   78477   15       1  13      111  2017-11-10 04:00:00             NaN   \n",
      "10004  123080   12       1  13      328  2017-11-10 04:00:00             NaN   \n",
      "\n",
      "       is_attributed  click_id  hour  day  \n",
      "10000            NaN       0.0     4   10  \n",
      "10001            NaN       1.0     4   10  \n",
      "10002            NaN       2.0     4   10  \n",
      "10003            NaN       3.0     4   10  \n",
      "10004            NaN       4.0     4   10  \n",
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(sampled_train.head())\n",
    "print(sampled_train.shape)\n",
    "print(\"=\"*80)\n",
    "print(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  46\n",
      "valid size:  0\n",
      "test size :  10000\n"
     ]
    }
   ],
   "source": [
    "val = sampled_train[(len_train-25000):len_train]\n",
    "train = sampled_train[:(len_train-25000)]\n",
    "\n",
    "print(\"train size: \", len(train))\n",
    "print(\"valid size: \", len(val))\n",
    "print(\"test size : \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'is_attributed'\n",
    "\n",
    "# TODO:全特徴量を使う\n",
    "predictors = ['app', 'device', 'os', 'channel', 'hour', 'day','ip_app_count', 'ip_app_os_count',\n",
    "              'lda_ip_app_0', 'lda_ip_app_1', 'lda_ip_app_2', 'lda_ip_app_3', 'lda_ip_app_4',\n",
    "              'lda_ip_os_0', 'lda_ip_os_1', 'lda_ip_os_2', 'lda_ip_os_3', 'lda_ip_os_4',\n",
    "              'lda_ip_channel_0', 'lda_ip_channel_1', 'lda_ip_channel_2', 'lda_ip_channel_3', 'lda_ip_channel_4']\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   click_id\n",
      "0         0\n",
      "1         1\n",
      "2         2\n",
      "3         3\n",
      "4         4\n"
     ]
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "test_id = pd.read_csv(DATADIR/'test.csv')\n",
    "sub['click_id'] = test_id['click_id'].astype('int')\n",
    "del test_id\n",
    "gc.collect()\n",
    "\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lgb_modelfit_nocv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-31303824c2e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m bst = lgb_modelfit_nocv(params,\n\u001b[0m\u001b[1;32m     22\u001b[0m                         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgb_modelfit_nocv' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.15,\n",
    "    # 'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 7,  # 2^max_depth - 1\n",
    "    'max_depth': 3,  # -1 means no limit\n",
    "    # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'min_child_samples': 100,\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    # Subsample ratio of columns when constructing each tree.\n",
    "    'colsample_bytree': 0.9,\n",
    "    # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'min_child_weight': 0,\n",
    "    'scale_pos_weight': 99  # because training data is extremely unbalanced\n",
    "    }\n",
    "\n",
    "bst = lgb_modelfit_nocv(params,\n",
    "                        train,\n",
    "                        val,\n",
    "                        predictors,\n",
    "                        target,\n",
    "                        objective='binary',\n",
    "                        metrics='auc',\n",
    "                        early_stopping_rounds=30,\n",
    "                        verbose_eval=True,\n",
    "                        num_boost_round=30,\n",
    "                        categorical_features=categorical)\n",
    "\n",
    "print('[{}]: model training time'.format(time.time() - start_time))\n",
    "del train_df\n",
    "del val_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
