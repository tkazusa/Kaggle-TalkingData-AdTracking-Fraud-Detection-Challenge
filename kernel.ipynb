{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - gensim==3.7.1\n",
      "    - lightgbm==2.2.2\n",
      "    - pandas==0.24.2\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    smart_open-1.8.2           |             py_0          49 KB  conda-forge\n",
      "    bz2file-0.98               |             py_0           9 KB  conda-forge\n",
      "    gensim-3.7.1               |   py36he1b5a44_0        22.7 MB  conda-forge\n",
      "    lightgbm-2.2.2             |   py36hf484d3e_0         990 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        23.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    bz2file:         0.98-py_0             conda-forge\n",
      "    gensim:          3.7.1-py36he1b5a44_0  conda-forge\n",
      "    lightgbm:        2.2.2-py36hf484d3e_0  conda-forge\n",
      "    smart_open:      1.8.2-py_0            conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2019.1.23-0                       --> 2019.3.9-hecc5488_0   conda-forge\n",
      "    certifi:         2019.3.9-py36_0                   --> 2019.3.9-py36_0       conda-forge\n",
      "    openssl:         1.0.2r-h7b6447c_0                 --> 1.0.2r-h14c3975_0     conda-forge\n",
      "    pandas:          0.23.0-py36h637b7d7_0             --> 0.24.2-py36hf484d3e_0 conda-forge\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install --channel conda-forge --yes --quiet --file requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from gensim import corpora, models\n",
    "\n",
    "from models import LightGBM, Model\n",
    "\n",
    "DATADIR = Path('./input')\n",
    "\n",
    "tr_path = DATADIR / 'train.csv'\n",
    "test_path = DATADIR / 'test.csv'\n",
    "\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os','channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandasデータ型指定によるメモリ使用量の削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データサイズが大きいので`float64`や`int64`をなるべく使わずに最適な型を選ぶように変換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "        \n",
    "    Args:\n",
    "        df (pd.DataFrame): pd.DataFrame to be reduced memory usage.\n",
    "    Regurns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"create a dataframe and optimize its memory usage\n",
    "    Args:\n",
    "        filepath (str): Path to csv file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed for memory usage reduction.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 11285.64 MB\n",
      "Memory usage after optimization is: 3721.47 MB\n",
      "Decreased by 67.0%\n",
      "Memory usage of dataframe is 1003.52 MB\n",
      "Memory usage after optimization is: 323.35 MB\n",
      "Decreased by 67.8%\n"
     ]
    }
   ],
   "source": [
    "train_sm = load_data(tr_path)\n",
    "test_sm = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sm.to_csv(DATADIR / 'train_sm.csv', index=False)\n",
    "test_sm.to_csv(DATADIR / 'test_sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_sm, test_sm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and bind train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATADIR / 'train_sm.csv', usecols=train_cols, parse_dates=True, keep_date_col=True)\n",
    "len_train = len(train)\n",
    "\n",
    "test = pd.read_csv(DATADIR / 'test_sm.csv', usecols=test_cols, parse_dates=True, keep_date_col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_tr_test(train: pd.DataFrame, test: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Bind train and test data for features engineering.\n",
    "    Args:\n",
    "        train (pd.DataFrame): train data.\n",
    "        test (pd.DataFrame): test data.\n",
    "    Returns:\n",
    "        data (pd.DataFrame): binded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    len_train = len(train)\n",
    "    print('The initial size of the train set is', len_train)\n",
    "    print('Binding the training and test set together...')\n",
    "    data = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bind_tr_test(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "click_timeは`2017-11-10 04:00:00`の形なので日付と時間の特徴量を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" create datatime-based features 'hour' and 'day' from 'click_time' strings.\n",
    "    Args:\n",
    "        data (pd.DataFrame): data concatinated train and test datasets.\n",
    "    Returns:\n",
    "        data (pd.DataFrame): data datatime-based featuers are converted from 'click_time'\n",
    "    \"\"\"\n",
    "    data['hour'] = pd.to_datetime(data.click_time).dt.hour.astype('uint8')\n",
    "    data['day'] = pd.to_datetime(data.click_time).dt.day.astype('uint8')\n",
    "    data = data.drop(['click_time'], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_time_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ベーシックな処理\n",
    "  - five raw categorical features (ip, os, app, channel, device)  （単純に型をカテゴリ化）\n",
    "  - time categorical features (day, hour) \n",
    "  - some count features \n",
    "- web広告配信データ特有の特徴量\n",
    "  - five raw categorical features (ip, os, app, channel, device) に対し、以下の特徴量を作成 (全組み合わせ2^5 -1 = 31通り)\n",
    "  - click count within next one/six hours  (直後1 or 6時間以内のクリック数)\n",
    "  - forward/backward click time delta  (前後クリックまでの時差)\n",
    "  - average attributed ratio of past click (過去のCVレート)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_channels_features(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Create and save count-based features.\n",
    "    Args:\n",
    "        data (pd.DataFrame): data concatinated train and test datasets.\n",
    "    \"\"\"\n",
    "    print(\"Creating new count features: 'n_channels', 'ip_app_count', 'ip_app_os_count'...\")\n",
    "\n",
    "    # Create \"n_channels\" feature\n",
    "    print('Computing the number of channels associated with a given IP address within each hour...')\n",
    "    n_chans = data[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'n_channels'})\n",
    "    data = data.merge(n_chans, on=['ip', 'day', 'hour'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['n_channels'].astype('uint16').to_csv(\n",
    "        DATADIR/'n_channels.csv', header=True, index=False)\n",
    "    data = data.drop(['n_channels'], axis=1)\n",
    "\n",
    "    # Create \"ip_app_count\" feature\n",
    "    print('Computing the number of channels associated with a given IP address and app...')\n",
    "    n_chans = data[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_count'})\n",
    "    data = data.merge(n_chans, on=['ip', 'app'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_count'].astype('uint16').to_csv(DATADIR/'ip_app_count.csv', header=True, index=False)\n",
    "    data = data.drop(['ip_app_count'], axis=1)\n",
    "\n",
    "    # Create \"ip_app_os_count\"\n",
    "    print('Computing the number of channels associated with a given IP address, app, and os...')\n",
    "    n_chans = data[['ip', 'app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_os_count'})\n",
    "    data = data.merge(n_chans, on=['ip', 'app', 'os'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_os_count'].astype('uint16').to_csv(DATADIR/'ip_app_os_count.csv', header=True, index=False)\n",
    "    data = data.drop(['ip_app_os_count'], axis=1)\n",
    "\n",
    "    del data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_count_channels_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAを用いたカテゴリカルデータの埋め込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のデータはipやosなど、多数のカテゴリをを抱える特徴量がある。それ単体でも特徴なり得るが、任意のカテゴリがどのような意味を持つかについて、他の特徴の各カテゴリとの共起から情報を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LDA_features(df: pd.DataFrame, num_topics: int, column_pair: Tuple[str, str])-> None:\n",
    "    \"\"\" Create and save LDA feateures calculated with a pair of categorical features.\n",
    "    Args:\n",
    "        df (pd.DataFrame): data.\n",
    "        num_topics (int): num of topics for LDA.\n",
    "        column_pair (tuple): tuple of column name of df, e.g. (ip, app).\n",
    "    \"\"\"\n",
    "    col1, col2 = column_pair\n",
    "    print('pair of %s & %s count data is converting to LDA topics' % (col1, col2))\n",
    "    tmp_dict = {}\n",
    "    for v_col1, v_col2 in zip(df[col1], df[col2]):\n",
    "        tmp_dict.setdefault(v_col1, []).append(str(v_col2))\n",
    "\n",
    "    col1_list = list(tmp_dict.keys())\n",
    "    col2s_of_col1s_list = [[' '.join(tmp_dict[tokun])] for tokun in col1_list]\n",
    "\n",
    "    dictionary = corpora.Dictionary(col2s_of_col1s_list)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in col2s_of_col1s_list]\n",
    "\n",
    "    model = models.LdaModel(corpus,\n",
    "                            num_topics=num_topics,\n",
    "                            id2word=dictionary,\n",
    "                            random_state=3655\n",
    "                            )\n",
    "\n",
    "    features = np.array(model.get_document_topics(\n",
    "        corpus, minimum_probability=0))[:, :, 1]\n",
    "\n",
    "    column_name_list = [\"lda_%s_%s_\" % (col1, col2) + str(i) for i in range(5)]\n",
    "\n",
    "    df_features = pd.DataFrame(features, columns=column_name_list)\n",
    "    df_features[col1] = col1_list\n",
    "\n",
    "    df = pd.merge(df, df_features, on=col1, how='left')\n",
    "    del df_features\n",
    "    gc.collect()\n",
    "\n",
    "    datapath = \"lda_\" + col1 + \"_\" + col2 + \".csv\"\n",
    "    df[column_name_list].to_csv(DATADIR/datapath, header=True, index=False)\n",
    "\n",
    "def get_column_pairs(columns: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Get pairs of column names from given column name list.\n",
    "    Args:\n",
    "        columns (List[str]): column names.\n",
    "    Returns: \n",
    "        List[Tuple[str,str]]: list of tuples of peirs of column names.\n",
    "    \"\"\"\n",
    "    return [(col1, col2) for col1, col2 in itertools.product(columns, repeat=2) if col1 != col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ip', 'app', 'os', 'channel']\n",
    "column_pairs = get_column_pairs(columns)\n",
    "\n",
    "for pair in column_pairs:\n",
    "    create_LDA_features(data, num_topics=5, column_pair=pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不均衡データに対するNegative donwsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで作成した特徴量をロードし、一つのデータマートとしてマージ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"n_channels\", \"ip_app_count\", \"ip_app_os_count\"]\n",
    "lda_features = [\"lda_\" + pair[0] + \"_\" + pair[1] for pair in column_pairs]\n",
    "\n",
    "features.extend(lda_features)\n",
    "\n",
    "for feature in features:\n",
    "    featurepath = feature + '.csv'\n",
    "    df_feature = pd.read_csv(DATADIR/featurepath)\n",
    "    data = pd.concat([data, df_feature], axis=1)\n",
    "    del df_feature\n",
    "    gc.collect()\n",
    "    print(\"merged feature: %s\" % feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, len_train):\n",
    "    train = data[:len_train]\n",
    "    test = data[len_train:]\n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(data, len_train)\n",
    "test.to_csv(DATADIR/'test_features.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルサイズの削減とクラス不均衡な二値分類への対応として、学習データへNegativeDownSamplingを使用した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_down_sampling(data: pd.DataFrame, random_state: int, target_variable: str) -> pd.DataFrame:\n",
    "    \"\"\"Create balanced dataset by matching the number of samples in the minority class with a random sampling.\n",
    "    Args: \n",
    "        data (pd.DataFrame): inbalanced data.\n",
    "        random_state (int): random state for sampling.\n",
    "        target_bariable (str): target variable for balancing.\n",
    "    Returns:\n",
    "        pd.DataFrame: balanced dataset. \n",
    "    \"\"\"\n",
    "    positive_data = data[data[target_variable] == 1]\n",
    "    positive_ratio = float(len(positive_data)) / len(data)\n",
    "    negative_data = data[data[target_variable] == 0].sample(\n",
    "        frac=positive_ratio / (1 - positive_ratio), random_state=random_state)\n",
    "    return pd.concat([positive_data, negative_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train = negative_down_sampling(train, target_variable='is_attributed', random_state=3655)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = sampled_train[(len_train-25000):len_train]\n",
    "# train = sampled_train[:(len_train-25000)]\n",
    "\n",
    "len_train = len(sampled_train)\n",
    "\n",
    "val = sampled_train[(len_train-25):len_train]\n",
    "train = sampled_train[:(len_train-25)]\n",
    "\n",
    "print(\"train size: \", len(train))\n",
    "print(\"valid size: \", len(val))\n",
    "print(\"test size : \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'is_attributed'\n",
    "categorical_features = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "# TODO:全特徴量を使う\n",
    "predictors = ['app', 'device', 'os', 'channel', 'hour', 'day','ip_app_count', 'ip_app_os_count',\n",
    "              'lda_ip_app_0', 'lda_ip_app_1', 'lda_ip_app_2', 'lda_ip_app_3', 'lda_ip_app_4',\n",
    "              'lda_ip_os_0', 'lda_ip_os_1', 'lda_ip_os_2', 'lda_ip_os_3', 'lda_ip_os_4',\n",
    "              'lda_ip_channel_0', 'lda_ip_channel_1', 'lda_ip_channel_2', 'lda_ip_channel_3', 'lda_ip_channel_4']\n",
    "\n",
    "params  = {\n",
    "    \n",
    "        \"model_params\": {\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": [\"auc\"],\n",
    "            \"learning_rate\": 0.2,\n",
    "            \"num_leaves\": 50,\n",
    "            \"max_depth\": 5,\n",
    "            \"max_bin\": 100,\n",
    "            \"subsample\": 0.7,\n",
    "            \"subsample_freq\": 1,\n",
    "            \"min_child_samples\": 100,\n",
    "            \"min_child_weight\": 0,\n",
    "            \"validation_ratio\": 0.1,\n",
    "            \"verbose\": 0\n",
    "        },\n",
    "    \n",
    "        \"train_params\": {\n",
    "            \"num_boost_round\": 250,\n",
    "            \"early_stopping_rounds\": 10\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightGBM()\n",
    "booster, result = model.train_and_predict(train=train, \n",
    "                                          valid=val,\n",
    "                                          categorical_features=categorical_features,\n",
    "                                          target=target,\n",
    "                                          params=params)\n",
    "\n",
    "best_iteration = booster.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, val])\n",
    "booster = model.train_without_validation(train=train,\n",
    "                                         categorical_features=categorical_features,\n",
    "                                         target=target,\n",
    "                                         params=params,\n",
    "                                         best_iteration=best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提出用データの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "test_id = pd.read_csv(DATADIR/'test.csv')\n",
    "sub['click_id'] = test_id['click_id'].astype('int')\n",
    "del test_id\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATADIR/'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = booster.predict(test[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-35c5c1e62a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_attributed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3445\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3446\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3630\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "sub['is_attributed'] = [1 if i >  else 0 for i in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
