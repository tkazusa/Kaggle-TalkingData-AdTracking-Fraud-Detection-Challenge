{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/linux-64::blaze==0.11.3=py36h4e06776_0\n",
      "  - defaults/linux-64::jupyter==1.0.0=py36_4\n",
      "  - defaults/linux-64::seaborn==0.8.1=py36hfad7ec4_0\n",
      "  - defaults/linux-64::scikit-image==0.13.1=py36h14c3975_1\n"
     ]
    }
   ],
   "source": [
    "!conda install --channel conda-forge --yes --quiet --file requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from gensim import corpora, models\n",
    "\n",
    "from models import LightGBM, Model\n",
    "\n",
    "DATADIR = Path('./input')\n",
    "\n",
    "tr_path = DATADIR / 'train.csv'\n",
    "test_path = DATADIR / 'test.csv'\n",
    "\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os',\n",
    "              'channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandasデータ型指定によるメモリ使用量の削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データサイズが大きいので`float64`や`int64`をなるべく使わずに最適な型を選ぶように変換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "        \n",
    "    Args:\n",
    "        df (pd.DataFrame): pd.DataFrame to be reduced memory usage.\n",
    "    Regurns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"create a dataframe and optimize its memory usage\n",
    "    Args:\n",
    "        filepath (str): Path to csv file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed for memory usage reduction.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, parse_dates=True, keep_date_col=True).head(10000)\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sm = load_data(tr_path)\n",
    "# test_sm = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sm.to_csv(DATADIR / 'train_sm.csv', index=False)\n",
    "# test_sm.to_csv(DATADIR / 'test_sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and bind train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATADIR / 'train_sm.csv', parse_dates=True, keep_date_col=True)\n",
    "len_train = len(train)\n",
    "\n",
    "test = pd.read_csv(DATADIR / 'test_sm.csv', parse_dates=True, keep_date_col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_tr_test(train: pd.DataFrame, test: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Bind train and test data for features engineering.\n",
    "    Args:\n",
    "        train (pd.DataFrame): train data.\n",
    "        test (pd.DataFrame): test data.\n",
    "    Returns:\n",
    "        data (pd.DataFrame): binded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    len_train = len(train)\n",
    "    print('The initial size of the train set is', len_train)\n",
    "    print('Binding the training and test set together...')\n",
    "    data = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial size of the train set is 10000\n",
      "Binding the training and test set together...\n"
     ]
    }
   ],
   "source": [
    "data = bind_tr_test(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "click_timeは`2017-11-10 04:00:00`の形なので日付と時間の特徴量を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(data: pd.DataFrame):\n",
    "    print(\"Creating new time features: 'hour' and 'day'...\")\n",
    "    data['hour'] = pd.to_datetime(data.click_time).dt.hour.astype('uint8')\n",
    "    data['day'] = pd.to_datetime(data.click_time).dt.day.astype('uint8')\n",
    "\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new time features: 'hour' and 'day'...\n"
     ]
    }
   ],
   "source": [
    "data = create_time_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ベーシックな処理\n",
    "  - five raw categorical features (ip, os, app, channel, device)  （単純に型をカテゴリ化）\n",
    "  - time categorical features (day, hour) \n",
    "  - some count features \n",
    "- web広告配信データ特有の特徴量\n",
    "  - five raw categorical features (ip, os, app, channel, device) に対し、以下の特徴量を作成 (全組み合わせ2^5 -1 = 31通り)\n",
    "  - click count within next one/six hours  (直後1 or 6時間以内のクリック数)\n",
    "  - forward/backward click time delta  (前後クリックまでの時差)\n",
    "  - average attributed ratio of past click (過去のCVレート)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_channels_features(data):\n",
    "    print(\"Creating new count features: 'n_channels', 'ip_app_count', 'ip_app_os_count'...\")\n",
    "\n",
    "    print('Computing the number of channels associated with ')\n",
    "    print('a given IP address within each hour...')\n",
    "    print('一時間の中でIPアドレス毎のチャネル数を数えている')\n",
    "    n_chans = data[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'n_channels'})\n",
    "    print('Merging the channels data with the main data set...')\n",
    "    data = data.merge(n_chans, on=['ip', 'day', 'hour'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['n_channels'].astype('uint16').to_csv(\n",
    "        DATADIR/'n_channels.csv', header=True, index=False)\n",
    "    print(\"Saving the data\")\n",
    "    data.drop(['n_channels'], axis=1)\n",
    "\n",
    "    print('Computing the number of channels associated with ')\n",
    "    print('a given IP address and app...')\n",
    "    print('IPアドレス毎/app毎のチャネル数を数えている')\n",
    "    n_chans = data[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_count'})\n",
    "    print('Merging the channels data with the main data set...')\n",
    "    data = data.merge(n_chans, on=['ip', 'app'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_count'].astype('uint16').to_csv(\n",
    "        DATADIR/'ip_app_count.csv', header=True, index=False)\n",
    "    print(\"Saving the data\")\n",
    "    data.drop(['ip_app_count'], axis=1)\n",
    "\n",
    "    print('Computing the number of channels associated with ')\n",
    "    print('a given IP address, app, and os...')\n",
    "    print('IPアドレス毎/app毎/os毎のチャネル数を数えている')\n",
    "    n_chans = data[['ip', 'app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_os_count'})\n",
    "    print('Merging the channels data with the main data set...')\n",
    "    data = data.merge(n_chans, on=['ip', 'app', 'os'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_os_count'].astype('uint16').to_csv(\n",
    "        DATADIR/'ip_app_os_count.csv', header=True, index=False)\n",
    "    print(\"Saving the data\")\n",
    "    data.drop(['ip_app_os_count'], axis=1)\n",
    "\n",
    "    del data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new count features: 'n_channels', 'ip_app_count', 'ip_app_os_count'...\n",
      "Computing the number of channels associated with \n",
      "a given IP address within each hour...\n",
      "一時間の中でIPアドレス毎のチャネル数を数えている\n",
      "Merging the channels data with the main data set...\n",
      "Saving the data\n",
      "Computing the number of channels associated with \n",
      "a given IP address and app...\n",
      "IPアドレス毎/app毎のチャネル数を数えている\n",
      "Merging the channels data with the main data set...\n",
      "Saving the data\n",
      "Computing the number of channels associated with \n",
      "a given IP address, app, and os...\n",
      "IPアドレス毎/app毎/os毎のチャネル数を数えている\n",
      "Merging the channels data with the main data set...\n",
      "Saving the data\n"
     ]
    }
   ],
   "source": [
    "create_count_channels_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAを用いたカテゴリカルデータの埋め込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のデータはipやosなど、多数のカテゴリをを抱える特徴量がある。それ単体でも特徴なり得るが、任意のカテゴリがどのような意味を持つかについて、他の特徴の各カテゴリとの共起から情報を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LDA_features(df: pd.DataFrame, num_topics: int, column_pair: tuple)-> None:\n",
    "    \"\"\" Create LDA feateures calculated with a pair of categorical features\n",
    "    Args\n",
    "        df:\n",
    "        num_topics:\n",
    "        column_pair \n",
    "    \"\"\"\n",
    "    col1, col2 = column_pair\n",
    "    print('pair of %s & %s' % (col1, col2))\n",
    "    tmp_dict = {}\n",
    "    for v_col1, v_col2 in zip(df[col1], df[col2]):\n",
    "        tmp_dict.setdefault(v_col1, []).append(str(v_col2))\n",
    "\n",
    "    col1_list = list(tmp_dict.keys())\n",
    "    col2s_of_col1s_list = [[' '.join(tmp_dict[tokun])] for tokun in col1_list]\n",
    "\n",
    "    dictionary = corpora.Dictionary(col2s_of_col1s_list)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in col2s_of_col1s_list]\n",
    "\n",
    "    model = models.LdaModel(corpus,\n",
    "                            num_topics=num_topics,\n",
    "                            id2word=dictionary,\n",
    "                            random_state=3655\n",
    "                            )\n",
    "\n",
    "    features = np.array(model.get_document_topics(\n",
    "        corpus, minimum_probability=0))[:, :, 1]\n",
    "\n",
    "    column_name_list = [\"lda_%s_%s_\" % (col1, col2) + str(i) for i in range(5)]\n",
    "\n",
    "    df_features = pd.DataFrame(features, columns=column_name_list)\n",
    "    df_features[col1] = col1_list\n",
    "\n",
    "    df = pd.merge(df, df_features, on=col1, how='left')\n",
    "    del df_features\n",
    "    gc.collect()\n",
    "\n",
    "    datapath = \"lda_\" + col1 + \"_\" + col2 + \".csv\"\n",
    "    df[column_name_list].to_csv(DATADIR/datapath, header=True, index=False)\n",
    "\n",
    "    print(\"Shape of merged data is %s %s \" % df[column_name_list].shape)\n",
    "\n",
    "\n",
    "def get_column_pairs(columns):\n",
    "    return [(col1, col2) for col1, col2 in itertools.product(columns, repeat=2) if col1 != col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair of ip & app\n",
      "Shape of merged data is 20000 5 \n",
      "pair of ip & os\n",
      "Shape of merged data is 20000 5 \n",
      "pair of ip & channel\n",
      "Shape of merged data is 20000 5 \n",
      "pair of app & ip\n",
      "Shape of merged data is 20000 5 \n",
      "pair of app & os\n",
      "Shape of merged data is 20000 5 \n",
      "pair of app & channel\n",
      "Shape of merged data is 20000 5 \n",
      "pair of os & ip\n",
      "Shape of merged data is 20000 5 \n",
      "pair of os & app\n",
      "Shape of merged data is 20000 5 \n",
      "pair of os & channel\n",
      "Shape of merged data is 20000 5 \n",
      "pair of channel & ip\n",
      "Shape of merged data is 20000 5 \n",
      "pair of channel & app\n",
      "Shape of merged data is 20000 5 \n",
      "pair of channel & os\n",
      "Shape of merged data is 20000 5 \n"
     ]
    }
   ],
   "source": [
    "columns = ['ip', 'app', 'os', 'channel']\n",
    "column_pairs = get_column_pairs(columns)\n",
    "\n",
    "for pair in column_pairs:\n",
    "    create_LDA_features(data, num_topics=5, column_pair=pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不均衡データに対するNegative donwsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで作成した特徴量をロードし、一つのデータマートとしてマージ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging n_channels\n",
      "   n_channels\n",
      "0           1\n",
      "1           1\n",
      "2           1\n",
      "3           1\n",
      "4           1\n",
      "shape of data is 20000 33\n",
      "merging ip_app_count\n",
      "   ip_app_count\n",
      "0             1\n",
      "1             2\n",
      "2             1\n",
      "3             5\n",
      "4             1\n",
      "shape of data is 20000 34\n",
      "merging ip_app_os_count\n",
      "   ip_app_os_count\n",
      "0                1\n",
      "1                1\n",
      "2                1\n",
      "3                2\n",
      "4                1\n",
      "shape of data is 20000 35\n",
      "merging lda_ip_app\n",
      "   lda_ip_app_0  lda_ip_app_1  lda_ip_app_2  lda_ip_app_3  lda_ip_app_4\n",
      "0      0.599627      0.100010      0.100359      0.100001      0.100002\n",
      "1      0.100068      0.599767      0.100004      0.100156      0.100004\n",
      "2      0.599604      0.100010      0.100383      0.100001      0.100002\n",
      "3      0.100351      0.100358      0.100371      0.100342      0.598577\n",
      "4      0.599627      0.100010      0.100359      0.100001      0.100002\n",
      "shape of data is 20000 40\n",
      "merging lda_ip_os\n",
      "   lda_ip_os_0  lda_ip_os_1  lda_ip_os_2  lda_ip_os_3  lda_ip_os_4\n",
      "0     0.100000     0.100023     0.599975     0.100000     0.100000\n",
      "1     0.599369     0.100582     0.100004     0.100042     0.100004\n",
      "2     0.100000     0.100023     0.599975     0.100000     0.100000\n",
      "3     0.100369     0.100424     0.100391     0.100426     0.598391\n",
      "4     0.100000     0.100023     0.599975     0.100000     0.100000\n",
      "shape of data is 20000 45\n",
      "merging lda_ip_channel\n",
      "   lda_ip_channel_0  lda_ip_channel_1  lda_ip_channel_2  lda_ip_channel_3  \\\n",
      "0          0.100012          0.598350          0.101596          0.100032   \n",
      "1          0.101124          0.100020          0.100478          0.598356   \n",
      "2          0.100012          0.598347          0.101598          0.100032   \n",
      "3          0.598585          0.100348          0.100327          0.100365   \n",
      "4          0.100012          0.598340          0.101606          0.100032   \n",
      "\n",
      "   lda_ip_channel_4  \n",
      "0          0.100011  \n",
      "1          0.100022  \n",
      "2          0.100011  \n",
      "3          0.100376  \n",
      "4          0.100011  \n",
      "shape of data is 20000 50\n",
      "merging lda_app_ip\n",
      "   lda_app_ip_0  lda_app_ip_1  lda_app_ip_2  lda_app_ip_3  lda_app_ip_4\n",
      "0      0.100031      0.100039      0.100034      0.599866      0.100031\n",
      "1      0.100031      0.100039      0.100034      0.599866      0.100031\n",
      "2      0.100031      0.100039      0.100034      0.599866      0.100031\n",
      "3      0.100030      0.100036      0.100030      0.100027      0.599877\n",
      "4      0.100031      0.100039      0.100034      0.599866      0.100031\n",
      "shape of data is 20000 55\n",
      "merging lda_app_os\n",
      "   lda_app_os_0  lda_app_os_1  lda_app_os_2  lda_app_os_3  lda_app_os_4\n",
      "0      0.100042      0.100042      0.100029      0.599855      0.100033\n",
      "1      0.100042      0.100042      0.100029      0.599855      0.100033\n",
      "2      0.100042      0.100042      0.100029      0.599855      0.100033\n",
      "3      0.100040      0.100038      0.100024      0.100026      0.599871\n",
      "4      0.100042      0.100042      0.100029      0.599855      0.100033\n",
      "shape of data is 20000 60\n",
      "merging lda_app_channel\n",
      "   lda_app_channel_0  lda_app_channel_1  lda_app_channel_2  lda_app_channel_3  \\\n",
      "0           0.100037           0.100042           0.100034           0.599862   \n",
      "1           0.100037           0.100042           0.100034           0.599862   \n",
      "2           0.100037           0.100042           0.100034           0.599862   \n",
      "3           0.100040           0.100043           0.100033           0.599857   \n",
      "4           0.100037           0.100042           0.100034           0.599862   \n",
      "\n",
      "   lda_app_channel_4  \n",
      "0           0.100026  \n",
      "1           0.100026  \n",
      "2           0.100026  \n",
      "3           0.100027  \n",
      "4           0.100026  \n",
      "shape of data is 20000 65\n",
      "merging lda_os_ip\n",
      "   lda_os_ip_0  lda_os_ip_1  lda_os_ip_2  lda_os_ip_3  lda_os_ip_4\n",
      "0     0.100029     0.100033     0.599880     0.100025     0.100033\n",
      "1     0.599884     0.100034     0.100028     0.100024     0.100030\n",
      "2     0.100029     0.100033     0.599880     0.100025     0.100033\n",
      "3     0.100029     0.100033     0.599880     0.100025     0.100033\n",
      "4     0.100029     0.100033     0.599880     0.100025     0.100033\n",
      "shape of data is 20000 70\n",
      "merging lda_os_app\n",
      "   lda_os_app_0  lda_os_app_1  lda_os_app_2  lda_os_app_3  lda_os_app_4\n",
      "0      0.100033      0.100037      0.100034      0.599864      0.100032\n",
      "1      0.599884      0.100032      0.100029      0.100025      0.100030\n",
      "2      0.100033      0.100037      0.100034      0.599864      0.100032\n",
      "3      0.100033      0.100037      0.100034      0.599864      0.100032\n",
      "4      0.100033      0.100037      0.100034      0.599864      0.100032\n",
      "shape of data is 20000 75\n",
      "merging lda_os_channel\n",
      "   lda_os_channel_0  lda_os_channel_1  lda_os_channel_2  lda_os_channel_3  \\\n",
      "0          0.100031          0.100032          0.599871          0.100030   \n",
      "1          0.100031          0.100031          0.100026          0.599881   \n",
      "2          0.100031          0.100032          0.599871          0.100030   \n",
      "3          0.100031          0.100032          0.599871          0.100030   \n",
      "4          0.100031          0.100032          0.599871          0.100030   \n",
      "\n",
      "   lda_os_channel_4  \n",
      "0          0.100035  \n",
      "1          0.100031  \n",
      "2          0.100035  \n",
      "3          0.100035  \n",
      "4          0.100035  \n",
      "shape of data is 20000 80\n",
      "merging lda_channel_ip\n",
      "   lda_channel_ip_0  lda_channel_ip_1  lda_channel_ip_2  lda_channel_ip_3  \\\n",
      "0          0.100033          0.100033          0.599863          0.100038   \n",
      "1          0.100033          0.100033          0.599863          0.100038   \n",
      "2          0.100033          0.100033          0.599863          0.100038   \n",
      "3          0.100029          0.100030          0.100024          0.100034   \n",
      "4          0.100033          0.100033          0.599863          0.100038   \n",
      "\n",
      "   lda_channel_ip_4  \n",
      "0          0.100033  \n",
      "1          0.100033  \n",
      "2          0.100033  \n",
      "3          0.599883  \n",
      "4          0.100033  \n",
      "shape of data is 20000 85\n",
      "merging lda_channel_app\n",
      "   lda_channel_app_0  lda_channel_app_1  lda_channel_app_2  lda_channel_app_3  \\\n",
      "0           0.100032           0.100030           0.100030           0.599872   \n",
      "1           0.100032           0.100030           0.100030           0.599872   \n",
      "2           0.100032           0.100030           0.100030           0.599872   \n",
      "3           0.599883           0.100028           0.100028           0.100027   \n",
      "4           0.100032           0.100030           0.100030           0.599872   \n",
      "\n",
      "   lda_channel_app_4  \n",
      "0           0.100036  \n",
      "1           0.100036  \n",
      "2           0.100036  \n",
      "3           0.100034  \n",
      "4           0.100036  \n",
      "shape of data is 20000 90\n",
      "merging lda_channel_os\n",
      "   lda_channel_os_0  lda_channel_os_1  lda_channel_os_2  lda_channel_os_3  \\\n",
      "0          0.100031          0.100029          0.100027          0.599878   \n",
      "1          0.100031          0.100029          0.100027          0.599878   \n",
      "2          0.100031          0.100029          0.100027          0.599878   \n",
      "3          0.599883          0.100028          0.100027          0.100028   \n",
      "4          0.100031          0.100029          0.100027          0.599878   \n",
      "\n",
      "   lda_channel_os_4  \n",
      "0          0.100035  \n",
      "1          0.100035  \n",
      "2          0.100035  \n",
      "3          0.100035  \n",
      "4          0.100035  \n",
      "shape of data is 20000 95\n"
     ]
    }
   ],
   "source": [
    "features = [\"n_channels\", \"ip_app_count\", \"ip_app_os_count\"]\n",
    "lda_features = [\"lda_\" + pair[0] + \"_\" + pair[1] for pair in column_pairs]\n",
    "\n",
    "features.extend(lda_features)\n",
    "\n",
    "for feature in features:\n",
    "    print(\"merging %s\" % feature)\n",
    "    featurepath = feature + '.csv'\n",
    "    df_feature = pd.read_csv(DATADIR/featurepath)\n",
    "    print(df_feature.head())\n",
    "    data = pd.concat([data, df_feature], axis=1)\n",
    "    del df_feature\n",
    "    gc.collect()\n",
    "    print(\"shape of data is %s %s\" % (data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>click_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>lda_channel_app_0</th>\n",
       "      <th>lda_channel_app_1</th>\n",
       "      <th>lda_channel_app_2</th>\n",
       "      <th>lda_channel_app_3</th>\n",
       "      <th>lda_channel_app_4</th>\n",
       "      <th>lda_channel_os_0</th>\n",
       "      <th>lda_channel_os_1</th>\n",
       "      <th>lda_channel_os_2</th>\n",
       "      <th>lda_channel_os_3</th>\n",
       "      <th>lda_channel_os_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>6481</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>173</td>\n",
       "      <td>2017-11-10 04:00:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9995.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100032</td>\n",
       "      <td>0.599875</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>0.100036</td>\n",
       "      <td>0.100032</td>\n",
       "      <td>0.599875</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>0.100028</td>\n",
       "      <td>0.100037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>10770</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-11-10 04:00:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9996.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100033</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.599871</td>\n",
       "      <td>0.100036</td>\n",
       "      <td>0.100031</td>\n",
       "      <td>0.100028</td>\n",
       "      <td>0.100028</td>\n",
       "      <td>0.599877</td>\n",
       "      <td>0.100036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>93325</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>140</td>\n",
       "      <td>2017-11-10 04:00:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9997.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100033</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.100031</td>\n",
       "      <td>0.599870</td>\n",
       "      <td>0.100035</td>\n",
       "      <td>0.100032</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>0.599876</td>\n",
       "      <td>0.100035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>17142</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>2017-11-10 04:00:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9998.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100034</td>\n",
       "      <td>0.599869</td>\n",
       "      <td>0.100031</td>\n",
       "      <td>0.100028</td>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.100034</td>\n",
       "      <td>0.599869</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>0.100038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>116535</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>265</td>\n",
       "      <td>2017-11-10 04:00:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100031</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.599879</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>0.100033</td>\n",
       "      <td>0.100033</td>\n",
       "      <td>0.100031</td>\n",
       "      <td>0.599870</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.100036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ip  app  device  os  channel           click_time attributed_time  \\\n",
       "19995    6481    3       1  13      173  2017-11-10 04:00:11             NaN   \n",
       "19996   10770   23       1  19      153  2017-11-10 04:00:11             NaN   \n",
       "19997   93325   12       1  19      140  2017-11-10 04:00:11             NaN   \n",
       "19998   17142   21       1   3      232  2017-11-10 04:00:11             NaN   \n",
       "19999  116535   12       1  13      265  2017-11-10 04:00:11             NaN   \n",
       "\n",
       "       is_attributed  click_id  hour  ...  lda_channel_app_0  \\\n",
       "19995            NaN    9995.0     4  ...           0.100032   \n",
       "19996            NaN    9996.0     4  ...           0.100033   \n",
       "19997            NaN    9997.0     4  ...           0.100033   \n",
       "19998            NaN    9998.0     4  ...           0.100034   \n",
       "19999            NaN    9999.0     4  ...           0.100031   \n",
       "\n",
       "       lda_channel_app_1  lda_channel_app_2  lda_channel_app_3  \\\n",
       "19995           0.599875           0.100030           0.100027   \n",
       "19996           0.100029           0.100030           0.599871   \n",
       "19997           0.100030           0.100031           0.599870   \n",
       "19998           0.599869           0.100031           0.100028   \n",
       "19999           0.100030           0.599879           0.100027   \n",
       "\n",
       "       lda_channel_app_4  lda_channel_os_0  lda_channel_os_1  \\\n",
       "19995           0.100036          0.100032          0.599875   \n",
       "19996           0.100036          0.100031          0.100028   \n",
       "19997           0.100035          0.100032          0.100029   \n",
       "19998           0.100037          0.100034          0.599869   \n",
       "19999           0.100033          0.100033          0.100031   \n",
       "\n",
       "       lda_channel_os_2  lda_channel_os_3  lda_channel_os_4  \n",
       "19995          0.100029          0.100028          0.100037  \n",
       "19996          0.100028          0.599877          0.100036  \n",
       "19997          0.100029          0.599876          0.100035  \n",
       "19998          0.100030          0.100029          0.100038  \n",
       "19999          0.599870          0.100030          0.100036  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, len_train):\n",
    "    train = data[:len_train]\n",
    "    test = data[len_train:]\n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(data, len_train)\n",
    "test.to_csv(DATADIR/'test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルサイズの削減とクラス不均衡な二値分類への対応として、学習データへNegativeDownSamplingを使用した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_down_sampling(data, random_state, target_variable):\n",
    "    positive_data = data[data[target_variable] == 1]\n",
    "    positive_ratio = float(len(positive_data)) / len(data)\n",
    "    negative_data = data[data[target_variable] == 0].sample(\n",
    "        frac=positive_ratio / (1 - positive_ratio), random_state=random_state)\n",
    "    return pd.concat([positive_data, negative_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_train = negative_down_sampling(train, target_variable='is_attributed', random_state=3655)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ip  app  device  os  channel           click_time  \\\n",
      "103   204158   35       1  13       21  2017-11-06 15:41:07   \n",
      "1504   29692    9       1  22      215  2017-11-06 16:00:02   \n",
      "1798   64516   35       1  13       21  2017-11-06 16:00:02   \n",
      "2102  172429   35       1  46      274  2017-11-06 16:00:03   \n",
      "3056  199085   35       1  13      274  2017-11-06 16:00:04   \n",
      "\n",
      "          attributed_time  is_attributed  click_id  hour  ...  \\\n",
      "103   2017-11-07 08:17:19            1.0       NaN    15  ...   \n",
      "1504  2017-11-07 10:05:22            1.0       NaN    16  ...   \n",
      "1798  2017-11-06 23:40:50            1.0       NaN    16  ...   \n",
      "2102  2017-11-07 00:55:29            1.0       NaN    16  ...   \n",
      "3056  2017-11-06 23:04:54            1.0       NaN    16  ...   \n",
      "\n",
      "      lda_ip_channel_1  lda_ip_channel_2  lda_ip_channel_3  lda_ip_channel_4  \\\n",
      "103           0.100004          0.100004          0.100004          0.599983   \n",
      "1504          0.100055          0.100001          0.599840          0.100001   \n",
      "1798          0.100412          0.100386          0.598292          0.100444   \n",
      "2102          0.100020          0.100019          0.100021          0.599916   \n",
      "3056          0.100020          0.100019          0.100021          0.599916   \n",
      "\n",
      "      lda_ip_os_0  lda_ip_os_1  lda_ip_os_2  lda_ip_os_3  lda_ip_os_4  \\\n",
      "103      0.100000     0.100023     0.599975     0.100000     0.100000   \n",
      "1504     0.100000     0.100015     0.100000     0.599984     0.100000   \n",
      "1798     0.100403     0.100462     0.598229     0.100464     0.100442   \n",
      "2102     0.100005     0.100005     0.599980     0.100005     0.100005   \n",
      "3056     0.100000     0.100023     0.599975     0.100000     0.100000   \n",
      "\n",
      "      n_channels  \n",
      "103            1  \n",
      "1504           1  \n",
      "1798           7  \n",
      "2102           1  \n",
      "3056           1  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "(46, 29)\n",
      "================================================================================\n",
      "           ip  app  device  os  channel           click_time attributed_time  \\\n",
      "10000    5744    9       1   3      107  2017-11-10 04:00:00             NaN   \n",
      "10001  119901    9       1   3      466  2017-11-10 04:00:00             NaN   \n",
      "10002   72287   21       1  19      128  2017-11-10 04:00:00             NaN   \n",
      "10003   78477   15       1  13      111  2017-11-10 04:00:00             NaN   \n",
      "10004  123080   12       1  13      328  2017-11-10 04:00:00             NaN   \n",
      "\n",
      "       is_attributed  click_id  hour  ...  lda_ip_channel_1  lda_ip_channel_2  \\\n",
      "10000            NaN       0.0     4  ...          0.100002          0.100002   \n",
      "10001            NaN       1.0     4  ...          0.598032          0.100440   \n",
      "10002            NaN       2.0     4  ...          0.100075          0.599680   \n",
      "10003            NaN       3.0     4  ...          0.100075          0.599680   \n",
      "10004            NaN       4.0     4  ...          0.100075          0.599680   \n",
      "\n",
      "       lda_ip_channel_3  lda_ip_channel_4  lda_ip_os_0  lda_ip_os_1  \\\n",
      "10000          0.100292          0.599700     0.100002     0.100003   \n",
      "10001          0.100491          0.100507     0.100330     0.100377   \n",
      "10002          0.100079          0.100081     0.599367     0.100583   \n",
      "10003          0.100079          0.100081     0.100000     0.100017   \n",
      "10004          0.100079          0.100081     0.100034     0.100001   \n",
      "\n",
      "       lda_ip_os_2  lda_ip_os_3  lda_ip_os_4  n_channels  \n",
      "10000     0.599990     0.100003     0.100003           2  \n",
      "10001     0.100349     0.598581     0.100363           9  \n",
      "10002     0.100004     0.100042     0.100004           2  \n",
      "10003     0.100004     0.100001     0.599978           4  \n",
      "10004     0.100000     0.100001     0.599964           2  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "(10000, 29)\n"
     ]
    }
   ],
   "source": [
    "print(sampled_train.head())\n",
    "print(sampled_train.shape)\n",
    "print(\"=\"*80)\n",
    "print(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = sampled_train[(len_train-25000):len_train]\n",
    "train = sampled_train[:(len_train-25000)]\n",
    "\n",
    "print(\"train size: \", len(train))\n",
    "print(\"valid size: \", len(val))\n",
    "print(\"test size : \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'is_attributed'\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "# TODO:全特徴量を使う\n",
    "predictors = ['app', 'device', 'os', 'channel', 'hour', 'day','ip_app_count', 'ip_app_os_count',\n",
    "              'lda_ip_app_0', 'lda_ip_app_1', 'lda_ip_app_2', 'lda_ip_app_3', 'lda_ip_app_4',\n",
    "              'lda_ip_os_0', 'lda_ip_os_1', 'lda_ip_os_2', 'lda_ip_os_3', 'lda_ip_os_4',\n",
    "              'lda_ip_channel_0', 'lda_ip_channel_1', 'lda_ip_channel_2', 'lda_ip_channel_3', 'lda_ip_channel_4']\n",
    "\n",
    "params  = {\n",
    "    \n",
    "        \"model_params\": {\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": [\"auc\"],\n",
    "            \"learning_rate\": 0.2,\n",
    "            \"num_leaves\": 50,\n",
    "            \"max_depth\": 5,\n",
    "            \"max_bin\": 100,\n",
    "            \"subsample\": 0.7,\n",
    "            \"subsample_freq\": 1,\n",
    "            \"min_child_samples\": 100,\n",
    "            \"min_child_weight\": 0,\n",
    "            \"validation_ratio\": 0.1,\n",
    "            \"verbose\": 0\n",
    "        },\n",
    "    \n",
    "        \"train_params\": {\n",
    "            \"num_boost_round\": 250,\n",
    "            \"early_stopping_rounds\": 10\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster, result = model.train_and_predict(train=train, \n",
    "                                          valid=val,\n",
    "                                          categorical_features=categorical_features,\n",
    "                                          target=target,\n",
    "                                          params=params)\n",
    "\n",
    "best_iteration = booster.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, val])\n",
    "booster = model.train_without_validation(train=train,\n",
    "                                         weight=None,\n",
    "                                         categorical_features=categorical_features,\n",
    "                                         target=target_variable,\n",
    "                                         params=params,\n",
    "                                         best_iteration=best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提出用データの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATADIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-69399405af03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'click_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'click_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATADIR' is not defined"
     ]
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "test_id = pd.read_csv(DATADIR/'test.csv')\n",
    "sub['click_id'] = test_id['click_id'].astype('int')\n",
    "del test_id\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATADIR/'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = booster.predict(test_data[predictors])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
