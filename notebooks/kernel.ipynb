{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talking Data AdTracking Fraud Detection Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要なパッケージのインストール。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --channel conda-forge --yes --quiet --file requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル構築開始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from gensim import corpora, models\n",
    "\n",
    "from models import LightGBM, Model\n",
    "\n",
    "DATADIR = Path('./input')\n",
    "\n",
    "tr_path = DATADIR / 'train.csv'\n",
    "test_path = DATADIR / 'test.csv'\n",
    "\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os','channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandasデータ型指定によるメモリ使用量の削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データサイズが大きいので`float64`や`int64`をなるべく使わずに最適な型を選ぶように変換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "        \n",
    "    Args:\n",
    "        df (pd.DataFrame): pd.DataFrame to be reduced memory usage.\n",
    "    Regurns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"create a dataframe and optimize its memory usage\n",
    "    Args:\n",
    "        filepath (str): Path to csv file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): pd.DataFrame which dtypes are changed for memory usage reduction.\n",
    "    \"\"\"\n",
    "    # df = pd.read_csv(filepath, parse_dates=True, keep_date_col=True) TODO: Use all data (memory size limitation)\n",
    "    df = pd.read_csv(filepath, parse_dates=True, keep_date_col=True).head(100000)\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 6.10 MB\n",
      "Memory usage after optimization is: 1.65 MB\n",
      "Decreased by 73.0%\n",
      "Memory usage of dataframe is 5.34 MB\n",
      "Memory usage after optimization is: 1.63 MB\n",
      "Decreased by 69.5%\n"
     ]
    }
   ],
   "source": [
    "train_sm = load_data(tr_path)\n",
    "test_sm = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sm.to_csv(DATADIR / 'train_sm.csv', index=False)\n",
    "test_sm.to_csv(DATADIR / 'test_sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_sm, test_sm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and bind train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATADIR / 'train_sm.csv', usecols=train_cols, parse_dates=True, keep_date_col=True)\n",
    "len_train = len(train)\n",
    "\n",
    "test = pd.read_csv(DATADIR / 'test_sm.csv', usecols=test_cols, parse_dates=True, keep_date_col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_tr_test(train: pd.DataFrame, test: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Bind train and test data for features engineering.\n",
    "    Args:\n",
    "        train (pd.DataFrame): train data.\n",
    "        test (pd.DataFrame): test data.\n",
    "    Returns:\n",
    "        data (pd.DataFrame): binded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    len_train = len(train)\n",
    "    print('The initial size of the train set is', len_train)\n",
    "    print('Binding the training and test set together...')\n",
    "    data = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial size of the train set is 100000\n",
      "Binding the training and test set together...\n"
     ]
    }
   ],
   "source": [
    "data = bind_tr_test(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "click_timeは`2017-11-10 04:00:00`の形なので日付と時間の特徴量を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" create datatime-based features 'hour' and 'day' from 'click_time' strings.\n",
    "    Args:\n",
    "        data (pd.DataFrame): data concatinated train and test datasets.\n",
    "    Returns:\n",
    "        data (pd.DataFrame): data datatime-based featuers are converted from 'click_time'\n",
    "    \"\"\"\n",
    "    data['hour'] = pd.to_datetime(data.click_time).dt.hour.astype('uint8')\n",
    "    data['day'] = pd.to_datetime(data.click_time).dt.day.astype('uint8')\n",
    "    data = data.drop(['click_time'], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_time_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ベーシックな処理\n",
    "  - five raw categorical features (ip, os, app, channel, device)  （単純に型をカテゴリ化）\n",
    "  - time categorical features (day, hour) \n",
    "  - some count features \n",
    "- web広告配信データ特有の特徴量\n",
    "  - five raw categorical features (ip, os, app, channel, device) に対し、以下の特徴量を作成 (全組み合わせ2^5 -1 = 31通り)\n",
    "  - click count within next one/six hours  (直後1 or 6時間以内のクリック数)\n",
    "  - forward/backward click time delta  (前後クリックまでの時差)\n",
    "  - average attributed ratio of past click (過去のCVレート)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_channels_features(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Create and save count-based features.\n",
    "    Args:\n",
    "        data (pd.DataFrame): data concatinated train and test datasets.\n",
    "    \"\"\"\n",
    "    print(\"Creating new count features: 'n_channels', 'ip_app_count', 'ip_app_os_count'...\")\n",
    "\n",
    "    # Create \"n_channels\" feature\n",
    "    print('Computing the number of channels associated with a given IP address within each hour...')\n",
    "    n_chans = data[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'n_channels'})\n",
    "    data = data.merge(n_chans, on=['ip', 'day', 'hour'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['n_channels'].astype('uint16').to_csv(\n",
    "        DATADIR/'n_channels.csv', header=True, index=False)\n",
    "    data = data.drop(['n_channels'], axis=1)\n",
    "\n",
    "    # Create \"ip_app_count\" feature\n",
    "    print('Computing the number of channels associated with a given IP address and app...')\n",
    "    n_chans = data[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_count'})\n",
    "    data = data.merge(n_chans, on=['ip', 'app'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_count'].astype('uint16').to_csv(DATADIR/'ip_app_count.csv', header=True, index=False)\n",
    "    data = data.drop(['ip_app_count'], axis=1)\n",
    "\n",
    "    # Create \"ip_app_os_count\"\n",
    "    print('Computing the number of channels associated with a given IP address, app, and os...')\n",
    "    n_chans = data[['ip', 'app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[\n",
    "        ['channel']].count().reset_index().rename(columns={'channel': 'ip_app_os_count'})\n",
    "    data = data.merge(n_chans, on=['ip', 'app', 'os'], how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    data['ip_app_os_count'].astype('uint16').to_csv(DATADIR/'ip_app_os_count.csv', header=True, index=False)\n",
    "    data = data.drop(['ip_app_os_count'], axis=1)\n",
    "\n",
    "    del data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new count features: 'n_channels', 'ip_app_count', 'ip_app_os_count'...\n",
      "Computing the number of channels associated with a given IP address within each hour...\n",
      "Computing the number of channels associated with a given IP address and app...\n",
      "Computing the number of channels associated with a given IP address, app, and os...\n"
     ]
    }
   ],
   "source": [
    "create_count_channels_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAを用いたカテゴリカルデータの埋め込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のデータはipやosなど、多数のカテゴリをを抱える特徴量がある。それ単体でも特徴なり得るが、任意のカテゴリがどのような意味を持つかについて、他の特徴の各カテゴリとの共起から情報を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LDA_features(df: pd.DataFrame, num_topics: int, column_pair: Tuple[str, str])-> None:\n",
    "    \"\"\" Create and save LDA feateures calculated with a pair of categorical features.\n",
    "    Args:\n",
    "        df (pd.DataFrame): data.\n",
    "        num_topics (int): num of topics for LDA.\n",
    "        column_pair (tuple): tuple of column name of df, e.g. (ip, app).\n",
    "    \"\"\"\n",
    "    col1, col2 = column_pair\n",
    "    print('pair of %s & %s count data is converting to LDA topics' % (col1, col2))\n",
    "    tmp_dict = {}\n",
    "    for v_col1, v_col2 in zip(df[col1], df[col2]):\n",
    "        tmp_dict.setdefault(v_col1, []).append(str(v_col2))\n",
    "\n",
    "    col1_list = list(tmp_dict.keys())\n",
    "    col2s_of_col1s_list = [[' '.join(tmp_dict[tokun])] for tokun in col1_list]\n",
    "\n",
    "    dictionary = corpora.Dictionary(col2s_of_col1s_list)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in col2s_of_col1s_list]\n",
    "\n",
    "    model = models.LdaModel(corpus,\n",
    "                            num_topics=num_topics,\n",
    "                            id2word=dictionary,\n",
    "                            random_state=3655\n",
    "                            )\n",
    "\n",
    "    features = np.array(model.get_document_topics(\n",
    "        corpus, minimum_probability=0))[:, :, 1]\n",
    "\n",
    "    column_name_list = [\"lda_%s_%s_\" % (col1, col2) + str(i) for i in range(5)]\n",
    "\n",
    "    df_features = pd.DataFrame(features, columns=column_name_list)\n",
    "    df_features[col1] = col1_list\n",
    "\n",
    "    df = pd.merge(df, df_features, on=col1, how='left')\n",
    "    del df_features\n",
    "    gc.collect()\n",
    "\n",
    "    datapath = \"lda_\" + col1 + \"_\" + col2 + \".csv\"\n",
    "    df[column_name_list].to_csv(DATADIR/datapath, header=True, index=False)\n",
    "\n",
    "def get_column_pairs(columns: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Get pairs of column names from given column name list.\n",
    "    Args:\n",
    "        columns (List[str]): column names.\n",
    "    Returns: \n",
    "        List[Tuple[str,str]]: list of tuples of peirs of column names.\n",
    "    \"\"\"\n",
    "    return [(col1, col2) for col1, col2 in itertools.product(columns, repeat=2) if col1 != col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair of ip & app count data is converting to LDA topics\n",
      "pair of ip & os count data is converting to LDA topics\n",
      "pair of ip & channel count data is converting to LDA topics\n",
      "pair of app & ip count data is converting to LDA topics\n",
      "pair of app & os count data is converting to LDA topics\n",
      "pair of app & channel count data is converting to LDA topics\n",
      "pair of os & ip count data is converting to LDA topics\n",
      "pair of os & app count data is converting to LDA topics\n",
      "pair of os & channel count data is converting to LDA topics\n",
      "pair of channel & ip count data is converting to LDA topics\n",
      "pair of channel & app count data is converting to LDA topics\n",
      "pair of channel & os count data is converting to LDA topics\n"
     ]
    }
   ],
   "source": [
    "columns = ['ip', 'app', 'os', 'channel']\n",
    "column_pairs = get_column_pairs(columns)\n",
    "\n",
    "for pair in column_pairs:\n",
    "    create_LDA_features(data, num_topics=5, column_pair=pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不均衡データに対するNegative donwsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで作成した特徴量をロードし、一つのデータマートとしてマージ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged feature: n_channels\n",
      "merged feature: ip_app_count\n",
      "merged feature: ip_app_os_count\n",
      "merged feature: lda_ip_app\n",
      "merged feature: lda_ip_os\n",
      "merged feature: lda_ip_channel\n",
      "merged feature: lda_app_ip\n",
      "merged feature: lda_app_os\n",
      "merged feature: lda_app_channel\n",
      "merged feature: lda_os_ip\n",
      "merged feature: lda_os_app\n",
      "merged feature: lda_os_channel\n",
      "merged feature: lda_channel_ip\n",
      "merged feature: lda_channel_app\n",
      "merged feature: lda_channel_os\n"
     ]
    }
   ],
   "source": [
    "features = [\"n_channels\", \"ip_app_count\", \"ip_app_os_count\"]\n",
    "lda_features = [\"lda_\" + pair[0] + \"_\" + pair[1] for pair in column_pairs]\n",
    "\n",
    "features.extend(lda_features)\n",
    "\n",
    "for feature in features:\n",
    "    featurepath = feature + '.csv'\n",
    "    df_feature = pd.read_csv(DATADIR/featurepath)\n",
    "    data = pd.concat([data, df_feature], axis=1)\n",
    "    del df_feature\n",
    "    gc.collect()\n",
    "    print(\"merged feature: %s\" % feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量を作成し終わったので、データをtrainとtestへ再び分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, len_train):\n",
    "    train = data[:len_train]\n",
    "    test = data[len_train:]\n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(data, len_train)\n",
    "test.to_csv(DATADIR/'test_features.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルサイズの削減とクラス不均衡な二値分類への対応として、学習データへNegativeDownSamplingを使用した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_down_sampling(data: pd.DataFrame, random_state: int, target_variable: str) -> pd.DataFrame:\n",
    "    \"\"\"Create balanced dataset by matching the number of samples in the minority class with a random sampling.\n",
    "    Args: \n",
    "        data (pd.DataFrame): inbalanced data.\n",
    "        random_state (int): random state for sampling.\n",
    "        target_bariable (str): target variable for balancing.\n",
    "    Returns:\n",
    "        pd.DataFrame: balanced dataset. \n",
    "    \"\"\"\n",
    "    positive_data = data[data[target_variable] == 1]\n",
    "    positive_ratio = float(len(positive_data)) / len(data)\n",
    "    negative_data = data[data[target_variable] == 0].sample(\n",
    "        frac=positive_ratio / (1 - positive_ratio), random_state=random_state)\n",
    "    return pd.concat([positive_data, negative_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_train = negative_down_sampling(train, target_variable='is_attributed', random_state=3655)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  313\n",
      "valid size:  25\n",
      "test size :  100000\n"
     ]
    }
   ],
   "source": [
    "# TODO: use all data \n",
    "# val = sampled_train[(len_train-25000):len_train] \n",
    "# train = sampled_train[:(len_train-25000)]\n",
    "\n",
    "len_train = len(sampled_train)\n",
    "\n",
    "val = sampled_train[(len_train-25):len_train]\n",
    "train = sampled_train[:(len_train-25)]\n",
    "\n",
    "print(\"train size: \", len(train))\n",
    "print(\"valid size: \", len(val))\n",
    "print(\"test size : \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'is_attributed'\n",
    "categorical_features = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "predictors =  ['ip', 'app', 'device', 'os', 'channel', 'hour', 'day',\n",
    "               'n_channels', 'ip_app_count', 'ip_app_os_count',\n",
    "               'lda_ip_app_0', 'lda_ip_app_1', 'lda_ip_app_2', 'lda_ip_app_3', 'lda_ip_app_4',\n",
    "               'lda_ip_os_0', 'lda_ip_os_1', 'lda_ip_os_2', 'lda_ip_os_3', 'lda_ip_os_4',\n",
    "               'lda_ip_channel_0', 'lda_ip_channel_1', 'lda_ip_channel_2', 'lda_ip_channel_3', 'lda_ip_channel_4',\n",
    "               'lda_app_ip_0', 'lda_app_ip_1', 'lda_app_ip_2', 'lda_app_ip_3', 'lda_app_ip_4', \n",
    "               'lda_app_os_0', 'lda_app_os_1', 'lda_app_os_2', 'lda_app_os_3', 'lda_app_os_4',\n",
    "               'lda_app_channel_0', 'lda_app_channel_1', 'lda_app_channel_2', 'lda_app_channel_3', 'lda_app_channel_4',\n",
    "               'lda_os_ip_0', 'lda_os_ip_1', 'lda_os_ip_2', 'lda_os_ip_3', 'lda_os_ip_4',\n",
    "               'lda_os_app_0', 'lda_os_app_1', 'lda_os_app_2', 'lda_os_app_3', 'lda_os_app_4',\n",
    "               'lda_os_channel_0', 'lda_os_channel_1', 'lda_os_channel_2', 'lda_os_channel_3', 'lda_os_channel_4',\n",
    "               'lda_channel_ip_0', 'lda_channel_ip_1', 'lda_channel_ip_2', 'lda_channel_ip_3', 'lda_channel_ip_4',\n",
    "               'lda_channel_app_0', 'lda_channel_app_1', 'lda_channel_app_2', 'lda_channel_app_3', 'lda_channel_app_4',\n",
    "               'lda_channel_os_0', 'lda_channel_os_1', 'lda_channel_os_2', 'lda_channel_os_3', 'lda_channel_os_4']\n",
    "\n",
    "params  = {\n",
    "    \n",
    "        \"model_params\": {\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": [\"auc\"],\n",
    "            \"learning_rate\": 0.2,\n",
    "            \"num_leaves\": 50,\n",
    "            \"max_depth\": 5,\n",
    "            \"max_bin\": 100,\n",
    "            \"subsample\": 0.7,\n",
    "            \"subsample_freq\": 1,\n",
    "            \"min_child_samples\": 100,\n",
    "            \"min_child_weight\": 0,\n",
    "            \"validation_ratio\": 0.1,\n",
    "            \"verbose\": 0\n",
    "        },\n",
    "    \n",
    "        \"train_params\": {\n",
    "            \"num_boost_round\": 250,\n",
    "            \"early_stopping_rounds\": 10\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's auc: 0.759862\tvalid's auc: 1\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttrain's auc: 0.844243\tvalid's auc: 1\n",
      "[3]\ttrain's auc: 0.836662\tvalid's auc: 1\n",
      "[4]\ttrain's auc: 0.844305\tvalid's auc: 1\n",
      "[5]\ttrain's auc: 0.836662\tvalid's auc: 1\n",
      "[6]\ttrain's auc: 0.871877\tvalid's auc: 1\n",
      "[7]\ttrain's auc: 0.884903\tvalid's auc: 1\n",
      "[8]\ttrain's auc: 0.885108\tvalid's auc: 1\n",
      "[9]\ttrain's auc: 0.883116\tvalid's auc: 1\n",
      "[10]\ttrain's auc: 0.889403\tvalid's auc: 1\n",
      "[11]\ttrain's auc: 0.898032\tvalid's auc: 1\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's auc: 0.759862\tvalid's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/lightgbm/basic.py:1190: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['app', 'channel', 'day', 'device', 'hour', 'os']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    }
   ],
   "source": [
    "model = LightGBM()\n",
    "booster, result = model.train_and_predict(train=train, \n",
    "                                          valid=val,\n",
    "                                          categorical_features=categorical_features,\n",
    "                                          target=target,\n",
    "                                          params=params)\n",
    "\n",
    "best_iteration = booster.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainとvalを合わせたデータで再び学習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/lightgbm/basic.py:1190: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['app', 'channel', 'day', 'device', 'hour', 'os']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([train, val])\n",
    "booster = model.train_without_validation(train=sampled_train,\n",
    "                                         categorical_features=categorical_features,\n",
    "                                         target=target,\n",
    "                                         params=params,\n",
    "                                         best_iteration=best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータを呼び出し、学習済モデルで予測を実施。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATADIR/'test_features.csv')\n",
    "prediction = booster.predict(test[predictors])\n",
    "threshold = 0.5 #TODO: find an appropriate threshold value.\n",
    "sub_is_attributed = [1 if i > threshold else 0 for i in prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提出用データの作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "test_id = pd.read_csv(DATADIR/'test.csv')\n",
    "sub['click_id'] = test_id['click_id'].astype('int')\n",
    "del test_id\n",
    "gc.collect()\n",
    "\n",
    "sub['is_attrobited'] = sub_is_attributed\n",
    "sub.to_csv('submission.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
